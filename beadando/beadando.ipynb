{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71106539-021d-4a5c-bb44-48f953bd2e46",
   "metadata": {},
   "source": [
    "## Szöveggenerálás a 'Cunk on Earth' című brit sorozat feliratfájljainak felhasználásával háromféle modellel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cac5b26-c6bf-44e0-9f59-8f144a7367a8",
   "metadata": {},
   "source": [
    "#### Készítette: Mészáros Dominik\n",
    "#### Neptun kód: EZU0EX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508289d4-82e2-4816-8905-268a1b48a6ed",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1185aff-3e07-4716-8533-41d2b6a58479",
   "metadata": {},
   "source": [
    "## Első modell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef602fa-7e40-4121-b421-6917f14d6c1a",
   "metadata": {},
   "source": [
    "##### A modell létrehozásához szükségek modulok importálása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eeef95-8dbd-4d91-b6b1-033e21b0ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394d0370-50ff-4fde-8d92-94ff495c20e3",
   "metadata": {},
   "source": [
    "##### A felhasználandó szöveg betöltése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed1cdb-e512-4844-a4df-9be1e3f1a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/data.txt\", \"r\") as f:\n",
    "    text = f.read().lower()\n",
    "print(len(text))\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9244ef53-98b0-4ce4-b193-56f40d7137b7",
   "metadata": {},
   "source": [
    "##### Adott hosszúságú szövegrészleteket adott számú eltolással egy listába rak,\n",
    "##### majd a szövegrészleteket követő karaktereket egy újabb listába helyez\n",
    "##### Ez azért kell, hogy a modell ne az egész szöveget egyszerre akarja feldolgozni, hanem részenként, de mégis legyen egy szövegkörnyezet amiben egy adott szövegrész beletartozik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6e60d5-09ee-41d9-bc70-947be1679e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 60\n",
    "step = 3\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen):\n",
    "    sequences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "len(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ab2c4c-bb1a-4b0b-81a6-df15f23f86a9",
   "metadata": {},
   "source": [
    "##### Minden egyedi karakterhez egy azonosítót tarsít"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e0e935-514a-4157-9f10-236315f971bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "char_index = dict((char, chars.index(char)) for char in chars)\n",
    "index_char = dict((chars.index(char), char) for char in chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb12a5e8-f758-4726-9915-36dd53f37124",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    x.append([char_index[char] for char in sequence])\n",
    "    y.append([char_index[next_chars[i]]])\n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15244888-5f2c-4cb8-aa2c-68960495b583",
   "metadata": {},
   "source": [
    "##### Létrehozzuk a modellt, majd betanítjuk a betöltött adattal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbfc071-c6f3-4381-86d5-c393ce374910",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 8\n",
    "num_chars = len(chars)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(num_chars, emb_size, input_length=maxlen),\n",
    "    tf.keras.layers.Conv1D(32, 5, activation='relu'),    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=num_chars, activation='softmax')\n",
    "]) \n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc4ba7-42c1-4dc2-bac7-2f3a7dc232b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    history = model.fit(x, y, epochs=25, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a729c62-2df0-4d26-a0c5-1f23bab05f61",
   "metadata": {},
   "source": [
    "##### A training loss és epoch függvényének szemléltetése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe267d5e-ec7b-4387-97de-1ef0d37f594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='(training data)',color='blue')\n",
    "plt.title('Neural Network training loss')\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91231f33-a985-42ee-8cea-d502cc8ac8ba",
   "metadata": {},
   "source": [
    "##### A modell egy véletlenszerűen kiválasztott szövegrészletet folytat a tanultak alapján"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911d8bfe-b26e-4517-8b35-910e53418199",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = random.randint(0, len(sequences))\n",
    "base_text = sequences[start]\n",
    "base_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691f3259-6236-4da4-965a-3e9a8f23a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_char = 200\n",
    "new_text = base_text\n",
    "sys.stdout.write(new_text)\n",
    "\n",
    "for _ in range(next_char):\n",
    "    prediction = np.argmax(model.predict(np.array([[char_index[char] for char in new_text]]), verbose=0), axis=-1)\n",
    "    new_char = index_char[prediction[0]]\n",
    "    new_text = new_text[1:] + new_char\n",
    "    sys.stdout.write(new_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea14e5e-a45a-4150-983b-30ebfc128b24",
   "metadata": {},
   "source": [
    "## Konklúzió"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad780ef-ec26-41c5-ae1a-d77fde2e1e24",
   "metadata": {},
   "source": [
    "#### A modell 25 epoch tanítás után 0.4121 pontosságot (1.9330 loss) tudott elérni. A tizedik epoch után sokat lassul a tanulás. Sok a szóismétlés, nem tud új ötletekkel előállni, ugyanazokat a szókapcsolatokat ismétli. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591ee56-a5c8-4efd-9150-69d4c436fcb0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557b0eaa-c442-4e9e-bb1d-8fda61613f9f",
   "metadata": {},
   "source": [
    "## Második modell - Long Short-Term Memory használatával"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b080a063-731d-4631-9d00-198e8746933f",
   "metadata": {},
   "source": [
    "##### A modell létrehozásához szükséges modulok importálása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba91b3-c9aa-4a30-ace6-1e658b057a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5fd285-d9a9-4d32-9491-3cb77e8426e7",
   "metadata": {},
   "source": [
    "##### A felhasználandó szöveg betöltése, illetve tokenizációja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62be150c-86be-4caf-ac47-2d3da43e9bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/data.txt\", \"r\") as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "print(len(text))\n",
    "\n",
    "lines = text.split(\"\\n\")\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "num_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1242432-0547-43c2-bdf3-a2e5ca8387f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lines[3])\n",
    "tokenizer.texts_to_sequences([lines[3]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d181ee8b-d6a7-483c-8499-ea8e25113751",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "\n",
    "for line in lines:\n",
    "    tokens = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "    for i in range(1, len(tokens)):\n",
    "        input_sequences.append(tokens[:i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15618111-fd89-4457-b65b-3975bc29c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e873d0-9058-4951-9db5-2e01eb990b47",
   "metadata": {},
   "source": [
    "##### Az összes szekvenciát feltöltjük 0-val, hogy az összes array 16 elem hosszú legyen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e31ddb-09b3-4d9b-8967-d48907e6a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(i) for i in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "np.random.shuffle(input_sequences)\n",
    "X, y = input_sequences[:,:-1], np.expand_dims(input_sequences[:,-1], axis=1)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66732dd5-dd6d-4295-8966-4f1b7ca23411",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caf169d-5c2a-437b-a12a-cfeb80b7b48b",
   "metadata": {},
   "source": [
    "##### Létrehozzuk a modellt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41686f10-3728-4f11-b24e-a1459f8efa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 256\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(num_words, emb_size, input_length=max_sequence_len - 1),\n",
    "    tf.keras.layers.LSTM(120),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(256),\n",
    "    tf.keras.layers.Dense(units=num_words, activation='softmax')\n",
    "]) \n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c3a9b4-3449-42a1-a964-ff9357c4bda4",
   "metadata": {},
   "source": [
    "##### Betanítjuk a modellt a feldolgozott adatokkal\n",
    "##### Az EarlyStopping callback function abban segít, hogyha a modell már nem képes kevesebb veszteséget elérni, akkor a tanulás befejeződik, ezzel időt és erőforrást spórolva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208d65cc-0ab8-41b1-97cb-834dd96998c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = EarlyStopping(monitor=\"loss\", patience=3)\n",
    "history = model.fit(X, y, epochs=25, batch_size=128, verbose=1, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d122f-3e70-4fc6-b91b-7e9f5e535755",
   "metadata": {},
   "source": [
    "##### A training loss és epoch függvényének szemléltetése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aadc0e-5d44-483e-b409-003a5bb48c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='(training data)',color='blue')\n",
    "plt.title('Neural Network training loss')\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7270eae9-c90e-4c42-b8d7-33a68c156bad",
   "metadata": {},
   "source": [
    "##### A training accuracy és epoch függvényének szemléltetése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4393c378-8783-46b8-be10-5d103bede482",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='(training data)',color='blue')\n",
    "plt.title('Neural Network training accuracy')\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec5b9c-beab-41cf-871d-dfdd10b01222",
   "metadata": {},
   "source": [
    "##### A létrehozott modell egy véletlenszerű szövegrészletet fog folytatni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee07e9-2b25-40d0-a154-329d1c56a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = random.randint(0, len(sequences))\n",
    "base_text = sequences[start]\n",
    "base_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224634ae-d429-4fe2-bc93-ee617521bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = base_text\n",
    "next_words = 100\n",
    "\n",
    "print(text)\n",
    "for _ in range(next_words):\n",
    "    tokens = tokenizer.texts_to_sequences([text])[0]\n",
    "    tokens = pad_sequences([tokens], maxlen=max_sequence_len-1, padding='pre')   \n",
    "    predicted = np.argmax(model.predict(tokens, verbose=0), axis=-1)\n",
    " \n",
    "    output_word = \"\"\n",
    "\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "\n",
    "    sys.stdout.write(output_word + \" \")\n",
    "    text += \" \" + output_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe92de3-7900-44b1-baea-b144177ac29d",
   "metadata": {},
   "source": [
    "## Konklúzió"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8ba106-2681-4505-855c-d359fc5a41b2",
   "metadata": {},
   "source": [
    "#### A modell 25 epoch tanítás után 0.5613 pontosságot (1.9215 loss) tudott elérni. 2-3 szavas értelmes és nyelvtanilag helyes szókapcsolatokat könnyedén tud generálni, viszont ezekből ritkán vagy egyáltalán nem tudott koherens mondatokat létrehozni."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1586d6-d6bf-4ac9-a1a2-c05717042358",
   "metadata": {},
   "source": [
    "## Harmadik modell rekurrens neurális hálóval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39b2122-1ec7-4f3f-b942-e347fe2c7ff5",
   "metadata": {},
   "source": [
    "##### A modell elkészítéséhez szükséges modulok importálása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b049786a-ab27-420b-b22d-d2b12a716aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47213d94-9d57-4df1-8f41-27b16164feef",
   "metadata": {},
   "source": [
    "##### A tanulásra felhasználandó szöveg beolvasása, majd az első modellnél használt adatelőkészítési módszerrel feldolgozzuk a szöveget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d601c15a-2bce-402b-915d-2c7ff38bd4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/data.txt\", \"r\") as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a77c08-78a7-4c62-967c-eaa11b903669",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 60\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d85507b-c554-45db-a060-b1428fe056dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d6fe88-d74d-4d8e-9b2d-413fdd161ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_index = dict((char, chars.index(char)) for char in chars)\n",
    "print(char_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dea1e6-bc5e-4e67-bb8a-8ed4d7a6e89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_char = dict((chars.index(char), char) for char in chars)\n",
    "print(index_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896a91a7-e1d5-48f9-9cfd-0600ca37f100",
   "metadata": {},
   "source": [
    "##### A karaktereket vektorizáljuk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ef9177-873c-4140-8d97-af0364872196",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_index[char]] = 1\n",
    "    y[i, char_index[next_chars[i]]] = 1\n",
    "\n",
    "print(\"x\", x.shape)\n",
    "print(\"y\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccaf4a8-8985-45e5-b23b-5c59f539316f",
   "metadata": {},
   "source": [
    "##### Létrehozzuk a rekurrens neurális hálót, majd betanítjuk az adatokkal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aed405e-b239-41de-af35-1d174b2a4011",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.GRU(16, input_shape=(maxlen, len(chars))),\n",
    "    tf.keras.layers.Dense(units=len(chars), activation='softmax')\n",
    "]) \n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.01))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39250df6-1919-4733-a7b8-489fda94b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    history = model.fit(x, y, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48015e5b-d041-427c-8bd9-1bd30cf70894",
   "metadata": {},
   "source": [
    "##### A training loss és epoch függvényének szemléltetése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab34dd3-5e8c-4898-956a-3c105d59508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='(training data)',color='blue')\n",
    "plt.title('Neural Network training loss')\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad0ba52-9f0a-4a29-a2b7-746cbb7c0107",
   "metadata": {},
   "source": [
    "##### A létrehozott modell egy véletlenszerű szövegrészletet fog folytatni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e84c21-9f89-4089-9d0f-7c0964d2bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = random.randint(0, len(sequences))\n",
    "base_text = sequences[start]\n",
    "base_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5666b060-7dee-4dd0-b2e4-2d79e89bb34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc4ca4f-0476-4d93-9718-8f392f04a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "temperatures = [0.2, 0.5, 1.0, 1.2]\n",
    "gen_characters = 200\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(\"Temp: \", temp)\n",
    "    generated_text = base_text\n",
    "    print(generated_text)\n",
    "    for i in range(gen_characters):\n",
    "        sampled = np.zeros((1, maxlen, len(chars)))    \n",
    "        for t, char in enumerate(generated_text):\n",
    "            sampled[0, t, char_index[char]] = 1.          \n",
    "        \n",
    "        preds = model.predict(sampled, verbose=0)[0]      \n",
    "        \n",
    "        next_index = sample(preds, temp)\n",
    "        next_char = chars[next_index]\n",
    "\n",
    "        generated_text += next_char\n",
    "        generated_text = generated_text[1:]\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d3ac4b-2605-4029-9609-d111de4f420e",
   "metadata": {},
   "source": [
    "## Konklúzió"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0d375-318a-4385-b2b9-fe6a33eb27b7",
   "metadata": {},
   "source": [
    "#### A modell 10 epoch tanítás után 1.9405 veszteséget ért el. Különböző temperature-t használva különböző eredményt kapunk. Láthatjuk, hogy a túl kevés, és a túl sok se jó, ezért kell megtalálni egy középértéket, amit a legjobbnak találunk. Itt 4 darab temperature értéket néztünk meg: \n",
    "* 0.2 érték mellett értelmes szavakat generál a modellünk, viszont ezek rövidek és sokat ismétlődnek\n",
    "* 0.5 értéket nézve még mindig értelmes szavakat kapunk, hosszabakat is mint az előző példánál, viszont koherencia csak ritkán figyelhető meg\n",
    "* 1.0 értéknél is figyelhetünk meg értelmes szavakat, viszont a többi szó nem sorolható az emberi aggyal értelmezhető szavak közé\n",
    "* 1.2 értéket megadva már csak csekély számmal találhatunk értelmes szavakat, de ezek is csak rövid szavak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a744e37d-fb04-4ab6-8b10-10f4e807d4de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
